{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Semantic Segmentation Algorithm - v9: deploy\n",
    "\n",
    "post migration from [roofAI](https://github.com/kamangir/roofAI/tree/main/notebooks/sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install 'sagemaker>=2,<3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamangir/git/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config      - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config      - Not applying SDK defaults from location: /Users/kamangir/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŒ€  blue_sandbox-5.108.1, built on 12 January 2025, 13:36:05\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from blueflow import notebooks\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "from blue_options import string\n",
    "\n",
    "from blue_sandbox import fullname\n",
    "from blue_sandbox.logger import logger\n",
    "\n",
    "logger.info(f\"{fullname()}, built on {string.pretty_date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-01-12 06:05:43 Starting - Preparing the instances for training\n",
      "2025-01-12 06:05:43 Downloading - Downloading the training image\n",
      "2025-01-12 06:05:43 Training - Training image download completed. Training in progress.\n",
      "2025-01-12 06:05:43 Uploading - Uploading generated training model\n",
      "2025-01-12 06:05:43 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "# attach to a completed training job\n",
    "ss_estimator = sagemaker.estimator.Estimator.attach(\"sagesemseg-model-2025-01-11-22-00-08-bW-2025-01-12-06-00-08-928\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker hosted endpoint. This will allow us to make predictions (or inference) from the model.\n",
    "\n",
    "Note that we don't have to host on the same number or type of instances that we used to train, and can choose any SageMaker-supported instance type. Training is compute-heavy job that may have different infrastructure requirements than inference/hosting. In our case we chose the GPU-accelerated `ml.p3.2xlarge` instance to train, but will host the model on a lower cost-per-hour `ml.c5.xlarge` type - because we'll only be serving occasional requests.\n",
    "\n",
    "The endpoint deployment can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sagemaker Creating model with name: sagesemseg-model-2025-01-11-22-00-08-bW-2025-01-12-21-36-11-294\n",
      "sagemaker Creating endpoint-config with name sagesemseg-model-2025-01-11-22-00-08-bW-2025-01-12-21-36-11-294\n",
      "sagemaker Creating endpoint with name sagesemseg-model-2025-01-11-22-00-08-bW-2025-01-12-21-36-11-294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------"
     ]
    }
   ],
   "source": [
    "ss_predictor = ss_estimator.deploy(initial_instance_count=1, instance_type=\"ml.c5.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with Estimators & training jobs, we can instead attach to an existing Endpoint:\n",
    "# ss_predictor = sagemaker.predictor.Predictor(\"ss-notebook-demo-2020-10-29-07-23-03-086\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the trained model is deployed to an endpoint, we can use this endpoint for inference.\n",
    "\n",
    "To test it out, let us download an image from the web which the algorithm has so-far not seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_raw = \"data/test.jpg\"\n",
    "\n",
    "!wget -O $filename_raw https://upload.wikimedia.org/wikipedia/commons/b/b4/R1200RT_in_Hongkong.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale of the input image may affect the prediction results and latency, so we'll down-scale the raw image before sending it to our endpoint. You could experiment with different input resolutions (and aspect ratios) and see how the results change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "filename = \"data/test_resized.jpg\"\n",
    "width = 800\n",
    "\n",
    "im = PIL.Image.open(filename_raw)\n",
    "\n",
    "aspect = im.size[0] / im.size[1]\n",
    "\n",
    "im.thumbnail([width, int(width / aspect)], PIL.Image.ANTIALIAS)\n",
    "im.save(filename, \"JPEG\")\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The endpoint accepts images in formats similar to the ones found images in the training dataset. The input `Content-Type` should be `image/jpeg`, and the output `Accept` type can be either:\n",
    "\n",
    "- `image/png`, which produces an indexed-PNG segmentation mask as used in training: One predicted class ID per pixel... Or,\n",
    "- `application/x-protobuf`, which produces a 3D matrix giving the *confidence of each class*, for each pixel.\n",
    "\n",
    "In the SageMaker SDK, A `Predictor` has an associated **serializer** and **deserializer** which control how data gets translated to our API call, and loaded back into a Python result object.\n",
    "\n",
    "There are pre-built [serializers](https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html) and [deserializers](https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html) offered by the SDK, and we're free to define custom ones so long as they offer the same API.\n",
    "\n",
    "\n",
    "### Basic inference - class IDs PNG\n",
    "\n",
    "In our first example, we'll request the simple PNG response and would like to map those into pixel arrays (assigned class for each pixel)... So we'll write a custom deserializer for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ImageDeserializer(sagemaker.deserializers.BaseDeserializer):\n",
    "    \"\"\"Deserialize a PIL-compatible stream of Image bytes into a numpy pixel array\"\"\"\n",
    "\n",
    "    def __init__(self, accept=\"image/png\"):\n",
    "        self.accept = accept\n",
    "\n",
    "    @property\n",
    "    def ACCEPT(self):\n",
    "        return (self.accept,)\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Read a stream of bytes returned from an inference endpoint.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): A stream of bytes.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            mask: The numpy array of class labels per pixel\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return np.array(Image.open(stream))\n",
    "        finally:\n",
    "            stream.close()\n",
    "\n",
    "\n",
    "ss_predictor.deserializer = ImageDeserializer(accept=\"image/png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input our data is already stored as a JPEG file, so we'll use the built-in `IdentitySerializer` and feed it the file bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor.serializer = sagemaker.serializers.IdentitySerializer(\"image/jpeg\")\n",
    "\n",
    "with open(filename, \"rb\") as imfile:\n",
    "    imbytes = imfile.read()\n",
    "\n",
    "# Extension exercise: Could you write a custom serializer which takes a filename as input instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that configured, calling our endpoint is now simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cls_mask = ss_predictor.predict(imbytes)\n",
    "\n",
    "print(type(cls_mask))\n",
    "print(cls_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the segmentation mask.\n",
    "\n",
    "Since the raw value of each pixel is a small number (the class ID), we'll apply a [colormap](https://matplotlib.org/3.3.2/tutorials/colors/colormaps.html) to make it a bit more human readable and not just a black square!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cls_mask, cmap=\"jet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced inference - class probabilities matrix\n",
    "\n",
    "The second `Accept` type allows us to request all the class probabilities for each pixel.\n",
    "\n",
    "Our input processing will be unchanged, but we'll define a new custom Deserializer to unpack the *RecordIO-wrapped protobuf* content returned by the endpoint.\n",
    "\n",
    "This format takes a little more effort to convert into an array than the basic PNG response. In the code below, we:\n",
    "\n",
    "- Make use of `mxnet` to open the [RecordIO](http://mesos.apache.org/documentation/latest/recordio/) wrapping\n",
    "- Use the `record_pb2` utility from the SageMaker SDK to load the Record contents in [protocol buffers](https://github.com/protocolbuffers/protobuf) format\n",
    "- Find that the record contains two fields `shape` (the shape of the matrix) and `target` (the probability predictions).\n",
    "- Load the `target` matrix in usable numpy array format, and map its shape appropriately.\n",
    "\n",
    "\n",
    "What we receive back is a recordio-protobuf of probablities sent as a binary. It takes a little bit of effort to convert into a readable array. Let us convert them to numpy format. We can make use of `mxnet` that has the capability to read recordio-protobuf formats. Using this, we can convert the outcoming bytearray into numpy array.\n",
    "\n",
    "The protobuf array has two parts to it. The first part contains the shape of the output and the second contains the values of probabilites. Using the output shape, we can transform the probabilities into the shape of the image, so that we get a map of values. There typically is a singleton dimension since we are only inferring on one image. We can also remove that using the `squeeze` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tempfile\n",
    "\n",
    "import mxnet as mx\n",
    "from sagemaker.amazon.record_pb2 import Record\n",
    "\n",
    "\n",
    "class SSProtobufDeserializer(sagemaker.deserializers.BaseDeserializer):\n",
    "    \"\"\"Deserialize protobuf semantic segmentation response into a numpy array\"\"\"\n",
    "\n",
    "    def __init__(self, accept=\"application/x-protobuf\"):\n",
    "        self.accept = accept\n",
    "\n",
    "    @property\n",
    "    def ACCEPT(self):\n",
    "        return (self.accept,)\n",
    "\n",
    "    def deserialize(self, stream, content_type):\n",
    "        \"\"\"Read a stream of bytes returned from an inference endpoint.\n",
    "        Args:\n",
    "            stream (botocore.response.StreamingBody): A stream of bytes.\n",
    "            content_type (str): The MIME type of the data.\n",
    "        Returns:\n",
    "            mask: The numpy array of class confidences per pixel\n",
    "        \"\"\"\n",
    "        try:\n",
    "            rec = Record()\n",
    "            # mxnet.recordio can only read from files, not in-memory file-like objects, so we buffer the\n",
    "            # response stream to a file on disk and then read it back:\n",
    "            with tempfile.NamedTemporaryFile(mode=\"w+b\") as ftemp:\n",
    "                ftemp.write(stream.read())\n",
    "                ftemp.seek(0)\n",
    "                recordio = mx.recordio.MXRecordIO(ftemp.name, \"r\")\n",
    "                protobuf = rec.ParseFromString(recordio.read())\n",
    "            values = list(rec.features[\"target\"].float32_tensor.values)\n",
    "            shape = list(rec.features[\"shape\"].int32_tensor.values)\n",
    "            # We 'squeeze' away extra dimensions introduced by the fact that the model can operate on batches\n",
    "            # of images at a time:\n",
    "            shape = np.squeeze(shape)\n",
    "            mask = np.reshape(np.array(values), shape)\n",
    "            return np.squeeze(mask, axis=0)\n",
    "        finally:\n",
    "            stream.close()\n",
    "\n",
    "\n",
    "ss_predictor.deserializer = SSProtobufDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prob_mask = ss_predictor.predict(imbytes)\n",
    "\n",
    "print(type(prob_mask))\n",
    "print(prob_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assigned class labels from the previous method are equivalent to the *index of the maximum-confidence class*, for each pixel - so we should be able to reconstruct the same image as before by taking the `argmax` over the classes dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_mask_2 = np.argmax(prob_mask, axis=0)\n",
    "\n",
    "plt.imshow(cls_mask_2, cmap=\"jet\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time, we can also view the *probabilities* for a particular class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cls_id = 14  # (motorbike)\n",
    "plt.imshow(prob_mask[target_cls_id, :, :], cmap=\"inferno\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And perhaps generate an overlay image for easy human review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imarray = np.array(PIL.Image.open(filename)) / 255.0  # Convert image pixels from 0-255 to 0-1\n",
    "hilitecol = np.array((0.0, 1.0, 1.0, 1.0))  # Cyan, 100% opacity (RGBAlpha 0-1 range)\n",
    "\n",
    "# Red-shift our image to make the cyan highlights more obvious:\n",
    "imshifted = imarray.copy()\n",
    "imshifted[:, :, 1] *= 0.6\n",
    "imshifted[:, :, 2] *= 0.5\n",
    "\n",
    "# Construct a mask with alpha channel taken from the model result:\n",
    "hilitemask = np.tile(hilitecol[np.newaxis, np.newaxis, :], list(imarray.shape[:2]) + [1])\n",
    "hilitemask[:, :, 3] = prob_mask[target_cls_id, :, :]\n",
    "\n",
    "# Overlay the two images:\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "ax0.imshow(imarray)\n",
    "ax0.axis(\"off\")\n",
    "ax0.set_title(\"Original Image\")\n",
    "ax2.imshow(hilitemask)\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title(\"Highlight Mask\")\n",
    "\n",
    "ax1.imshow(imshifted)\n",
    "ax1.imshow(hilitemask)\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title(\"Color-shifted Overlay\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Deployed endpoints are backed by infrastructure (1x`ml.c5.xlarge` in our case, as we requested above) - so we should delete the endpoint when we're finished with it, to avoid incurring continued costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_predictor.delete_endpoint()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|semantic_segmentation_pascalvoc|semantic_segmentation_pascalvoc.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
